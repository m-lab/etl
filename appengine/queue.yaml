total_storage_limit: 2000M

queue:
- name: etl-ndt-queue
  target: etl-ndt-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  # 1.0 allow processing a day's data (about 11K tasks) in 3 to 4 hours.
  rate: 1.0/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 360

# Notes on performance tuning:
# https://docs.google.com/document/d/114sbHx4wNK3eWpM1OnVD_Qhlwp2Hdh-XlBI5zTuNQP8/edit#
#
# BigQuery has a streaming insert quota per table.  The published quota is
# 100MB/sec per table, but we have been hitting the quota at seemingly
# lower rates - around 200 rows/second for NDT with deltas, which is about
# 20MB/sec of data in the final table.
# Not clear what is going on - perhaps the quota is on the size of incoming
# HTTP request data.
#
# We will design to stay within about 200 rows/sec/table for inserts with deltas
# but we can run about 1200 rows/sec/table without deltas.  This unfortunately
# means that we should update the number of concurrent requests depending on
# whether we are doing full deltas.
# 
# Without deltas, we are observing about 4 instance-hours / day of data for
# mid 2017.  This means with 40 instances, we can process about 8 months/day.
# With deltas, this slows dramatically, to about 10-15 rows/sec/instance,
# or about 1 day/day/instance.
#
# For processing archival data, we want to be able to process roughly one month
# per day, which for currentdata rates of 1 million tests per day, is
# about 30 million rows per day, or about 350 rows/second.
#
# When reprocessing data older than 30 days, the pipeline inserts rows 
# into templated tables, which means there is a separate table per day.
# So for archive reprocessing more than 30 days in the past, the BQ quota
# ends up being rows/sec per day/date.
#
# With deltas, let's choose a conservative BQ rate limit of 200 rows/second,
# per table, and therefore per archive date.  Without deltas, we will
# design for about 1200 rows/second.
#
# The new scraper produces about 16K tasks per day, with about 60 row inserts
# per task.  But older archives are much larger, with perhaps 1000 to 2000 tar
# files per day, and 600 to 1000 tests per file.
# 
# We will limit the rate based on number of concurrent tasks, so we can
# regulate the throughput independent of the size of the archive files.
# With deltas, we want 200 rows/second, divided by 1.3 rows/sec per task,
# results in a target of roughly 150 concurrent tasks per archive date,
# or 1200 tasks across 8 queues.
#
# Without deltas, the throughput is much higher.  We will try with the
# same number of concurrent tasks, and adjust if needed.
#
# To reach the desired 350 rows/second required to process one month's
# data each day, we need to process at least 2 days of data in parallel.
# The quota gaurantee comes from putting any one day's data into a single
# task queue, and the desired aggregate throughput can be achieved by
# having multiple queues, and spraying different days across different
# queues.
#
# To make things conceptually simple, we will just create queues with
# suffixes like -0, -1, -2 ...
# A corresponding batch reprocessing script will use Gregorian ordinal
# of the date, mod N, to determine which queue to drop any given date
# into.  We just choose an appropriate N that gives us the throughput
# we need.  For now, N=8 gives us a bit of headroom, so this config
# sets up -0 through -7.
#
# The current pipeline config has up to 40 instances with 2 cpus and
# 12 workers each.  This could handle up to 0.9 * 40 * 12 or 432
# concurrent tasks, which is well in excess of what we currently need.
# Some adjustments may be needed once this is running regularly.
#
# SUMMARY:
# 1. Each queue must limit concurrent requests to about 55 to limit
#    rate into individual tables to around 200 rows/second (with deltas)
# 2. We have 8 queues, to allow up to about 1600 rows/second, to
#    allow 30 days of data to be processed within one day, with
#    some headroom for future growth.
- name: etl-ndt-batch-0
  target: etl-ndt-batch-parser
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 55
- name: etl-ndt-batch-1
  target: etl-ndt-batch-parser
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 55
- name: etl-ndt-batch-2
  target: etl-ndt-batch-parser
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 55
- name: etl-ndt-batch-3
  target: etl-ndt-batch-parser
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 55
- name: etl-ndt-batch-4
  target: etl-ndt-batch-parser
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 55
- name: etl-ndt-batch-5
  target: etl-ndt-batch-parser
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 55
- name: etl-ndt-batch-6
  target: etl-ndt-batch-parser
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 55
- name: etl-ndt-batch-7
  target: etl-ndt-batch-parser
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 55

- name: etl-traceroute-queue
  target: etl-traceroute-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  rate: 1.5/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 360

- name: etl-sidestream-queue
  target: etl-sidestream-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  rate: 1.5/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 180

- name: etl-disco-queue
  target: etl-disco-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  rate: 2/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 180

