total_storage_limit: 2000M

queue:
- name: etl-ndt-queue
  target: etl-ndt-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  # 2.0 is adequate to provide about 50% headroom for daily rate.
  rate: 2.0/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 360

# Notes on performance tuning:
# https://docs.google.com/document/d/114sbHx4wNK3eWpM1OnVD_Qhlwp2Hdh-XlBI5zTuNQP8/edit#
#
# BigQuery has a streaming insert quota per table.  The published quota is
# 100MB/sec per table, but we have been hitting the quota at seemingly
# lower rates - around 200 rows/second for NDT with deltas, which is about
# 20MB/sec of data in the final table.
# Not clear what is going on - perhaps the quota is on the size of incoming
# HTTP request data.
#
# We will design to stay within about 200 rows/sec/table for inserts with deltas
# but we can run about 1200 rows/sec/table without deltas.  This unfortunately
# means that we should update the number of concurrent requests depending on
# whether we are doing full deltas.
#
# Without deltas, we are observing about 4 instance-hours / day of data for
# mid 2017.  This means with 40 instances, we can process about 8 months/day.
# With deltas, this slows dramatically, to about 10-15 rows/sec/instance,
# or about 1 day/day/instance.
#
# For processing archival data, we want to be able to process roughly one month
# per day, which for currentdata rates of 1 million tests per day, is
# about 30 million rows per day, or about 350 rows/second.
#
# When reprocessing data older than 30 days, the pipeline inserts rows
# into templated tables, which means there is a separate table per day.
# So for archive reprocessing more than 30 days in the past, the BQ quota
# ends up being rows/sec per day/date.
#
# With deltas, let's choose a conservative BQ rate limit of 200 rows/second,
# per table, and therefore per archive date.  Without deltas, we will
# design for about 1200 rows/second.
#
# The new scraper produces about 16K tasks per day, with about 60 row inserts
# per task.  But older archives are much larger, with perhaps 1000 to 2000 tar
# files per day, and 600 to 1000 tests per file.
#
# We will limit the rate based on number of concurrent tasks, so we can
# regulate the throughput independent of the size of the archive files.
# With deltas, we want 200 rows/second, divided by 1.3 rows/sec per task,
# results in a target of roughly 150 concurrent tasks per archive date,
# or 1200 tasks across 8 queues.
#
# Without deltas, the throughput is much higher.  We will try with the
# same number of concurrent tasks, and adjust if needed.
#
# To reach the desired 350 NDT rows/second required to process one month's
# data each day, we need to process at least 2 days of data in parallel.
# The quota gaurantee comes from putting any one day's data into a single
# task queue, and the desired aggregate throughput can be achieved by
# having multiple queues, and spraying different days across different
# queues.
#
# To make things conceptually simple, we will just create queues with
# suffixes like -0, -1, -2 ...  Tasks for each date will be submitted
# to a single queue, and each queue will be fully drained before
# submitting a new day's tasks.
# For now, N=16 gives us a bit of headroom, so this config sets up -0 through -15.
#
# The current pipeline config has up to 40 instances with 2 cpus and
# 15 workers each.  This could handle about 0.9 * 40 * 15 or 560
# concurrent tasks, which is well in excess of what we currently need.
# Some adjustments may be needed once this is running regularly.
#
# SUMMARY:
# 1. Each queue must limit concurrent requests to about 40 to limit
#    rate into individual tables to around 200 rows/second (with deltas).
#    We actually limit to 40, so we don't overwhelm the number of workers.
# 2. We have 16 queues, to allow some queues to be empty and still maintain
#    nearly 100% cpu utilization across 40 instances.
# 3. Setting max_doublings to 0 means that backoff will increase linearly
#    by min_backoff_seconds each retry.
# 4. Some tasks are pathological, and may never complete.  Since we are
#    draining queues between date submissions, we expect a day's data to
#    complete within a few hours.  We set an expiration of 12 hours in
#    case some task consistently fails.
#    TODO - should check into whether limiting retries would be better.
#
- name: etl-ndt-batch-0
  target: etl-batch-parser
  # At 10/s, the ndt requests cause a lot of 429s, and crowd out the other queues.
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    # Setting max_doublings to 0 means that backoff will increase by min_backoff_seconds each retry.
    max_doublings: 0
- name: etl-ndt-batch-1
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-2
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-3
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-4
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-5
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-6
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-7
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-8
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-9
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-10
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-11
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-12
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-13
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-14
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt-batch-15
  target: etl-batch-parser
  rate: 3/s
  bucket_size: 10
  max_concurrent_requests: 120
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0

- name: etl-sidestream-batch-0
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-1
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-2
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-3
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-4
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-5
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-6
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-7
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-8
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-9
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-10
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-11
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-12
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-13
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-14
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-sidestream-batch-15
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because sidestream days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20

- name: etl-traceroute-batch-0
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because traceroute days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-1
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-2
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-3
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-4
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-5
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-6
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-7
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-8
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-9
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-10
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-11
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-12
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-13
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-14
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-traceroute-batch-15
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20

- name: etl-disco-batch-0
  target: etl-batch-parser
  rate: 2/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 5
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because disco days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-disco-batch-1
  target: etl-batch-parser
  rate: 2/s
  bucket_size: 10
  max_concurrent_requests: 5
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-disco-batch-2
  target: etl-batch-parser
  rate: 2/s
  bucket_size: 10
  max_concurrent_requests: 5
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-disco-batch-3
  target: etl-batch-parser
  rate: 2/s
  bucket_size: 10
  max_concurrent_requests: 5
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20

# TCPINFO batch parsing queues.
- name: etl-tcpinfo-batch-0
  target: etl-batch-parser
  rate: 2/s
  bucket_size: 10
  max_concurrent_requests: 200
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20

- name: etl-tcpinfo-batch-1
  target: etl-batch-parser
  rate: 2/s
  bucket_size: 10
  max_concurrent_requests: 200
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20

- name: etl-tcpinfo-batch-2
  target: etl-batch-parser
  rate: 2/s
  bucket_size: 10
  max_concurrent_requests: 200
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20

- name: etl-tcpinfo-batch-3
  target: etl-batch-parser
  rate: 2/s
  bucket_size: 10
  max_concurrent_requests: 200
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20

# NDT5
- name: etl-ndt5-batch-0
  target: etl-batch-parser
  # At 0.2/sec, this was getting crowded out by ndt.
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 10
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0
- name: etl-ndt5-batch-1
  target: etl-batch-parser
  rate: 5/s
  bucket_size: 10
  max_concurrent_requests: 10
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
    max_doublings: 0

# Scamper JSON barch queue.
- name: etl-scamper-batch-0
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  # max concurrent is limited, because we will share the pipeline with other experiment types
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    # min and max are same, so that queue rate only diminishes as tasks are drained
    # it is set rather low, because traceroute days have relatively few files compared to NDT
    # and we want the rate limited by the rate/concurrent requests, not by the backoff
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-scamper-batch-1
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-scamper-batch-2
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20
- name: etl-scamper-batch-3
  target: etl-batch-parser
  rate: 1/s
  bucket_size: 10
  max_concurrent_requests: 20
  retry_parameters:
    task_age_limit: 12h
    min_backoff_seconds: 20
    max_backoff_seconds: 20


- name: etl-traceroute-queue
  target: etl-traceroute-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  rate: 1/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 5

- name: etl-scamper-queue
  target: etl-scamper-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  rate: 1/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 5

- name: etl-sidestream-queue
  target: etl-sidestream-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  rate: 1.5/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 180

- name: etl-fast-ss-queue
  target: etl-fast-ss-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  rate: 1.5/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 120

- name: etl-disco-queue
  target: etl-disco-parser
  # Average rate at which to release tasks to the service.  Default is 5/sec
  # This is actually the rate at which tokens are added to the bucket.
  rate: 2/s
  # Number of tokens that can accumulate in the bucket.  Default is 5.  This should
  # have very little impact for our environment.
  bucket_size: 10
  # Maximum number of concurrent requests.
  max_concurrent_requests: 180

