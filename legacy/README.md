# Legacy schema and scripts.

Scripts and schema for handling legacy ndt tables generated by internal
pipeline.  These were originally stored in dremel and used the plx
bridge to provide a bigquery interface.

## Schema
common.json specifies a common schema that has all of the non-empty
legacy plx fields, and adds fields introduced with the ETL pipeline
schema.  The legacy fields differ only in making the top level log_time
into a timestamp, for ease of use and compatibility with the new
schema.  Note, however, that the log_time field is missing from MANY
legacy table rows.

The table is created with:
```bash
bq mk --time_partitioning_type=DAY --schema=common.json measurement-lab:legacy.ndt
```

There is currently a limit of 2500 partitions per table, so the legacy data is split across two tables.
measurement-lab:legacy.ndt contains data since January 2015, and
ndt_pre2015 contains all data prior from 2009 through 2015.

The table name is hard coded into the convert-legacy... script, and
just changed manually to split across the tables.  This seems ok,
since we are unlikely to do this more than once or twice.

## Scripts
The convert-legacy-to-common.sh script is used to copy a single day
from the old plx tables into a new, partitioned
measurement-lab:legacy.ndt table.  It provides the explicit
interleaving of legacy and new schema fields, generally adding
null values to the new fields, with a couple exceptions where the
syntax would be excessively verbose (task_filename and parse_time).

The convert-year.sh script runs copy queries for an entire year.  It
requests copies for 31 days each month, so months with fewer than 31 days __will exhibit failures__ for the days that don't exist.
These show up as 'Invalid timestamp string' errors.

The script runs a whole month of synchronous queries in parallel,
then does a __wait__ for all of them to complete before starting the
next month. 

I tried running a whole year of queries asynchronously, but this
resulted in a fairly high failed query rate.  The one month synchronous
strategy has worked without any failed queries so far.

Each individual query typically takes about a minute to process, but
bigquery queues them, and generally takes a minute or two before it starts processing.  It generally processes 15 or 20 in parallel, and
it takes on the order of an hour to process a full year's worth of
queries.

## Cost
IIUC, the cost to run these scripts is trivial.  The total data
queried and copied is on the order of 1 TB, which implies a cost
of around $5.00 for the entire fast table.

## Data

Note that the __partition_date__ field should not be relied on.  It was
added early in development, and overlooked later when it was no longer
needed.

__partition_date__ should instead be derived from the legacy.ndt
___partition_time__ field.

### Validation
The transfer ran into occasional errors, requiring some daily queries
to be repeated.  In the end, the new table was validated by comparing
the query counts, per year, against PLX tables, using these queries:

```sql
#standardSQL
# Query test_id counts per year in plx fast table
SELECT DATE_TRUNC(parse_date("%Y/%m/%d", substr(test_id, 1, 10)), year) as year, count(test_id) as tests
from `plx.google.m_lab.ndt.all`
group by year
order by year
```
```sql
#standardSQL
# Query test_id counts per year, across legacy.ndt and
# legacy.ndt_pre2015
select * from (
SELECT DATE_TRUNC(date(_partitiontime), year) as year, count(test_id) as tests
from `measurement-lab.legacy.ndt`
group by year

union all
SELECT DATE_TRUNC(date(_partitiontime), year) as year, count(test_id) as tests
from `measurement-lab.legacy.ndt_pre2015`
group by year
) order by year
```

These numbers match for all years 2009 through 2017.